
==> Audit <==
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    COMMAND     â”‚           ARGS           â”‚ PROFILE  â”‚ USER â”‚ VERSION â”‚     START TIME      â”‚      END TIME       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ start          â”‚ --driver=docker          â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:25 IST â”‚ 30 Nov 25 11:34 IST â”‚
â”‚ ip             â”‚                          â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:34 IST â”‚ 30 Nov 25 11:34 IST â”‚
â”‚ docker-env     â”‚ minikube docker-env      â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:34 IST â”‚ 30 Nov 25 11:34 IST â”‚
â”‚ update-context â”‚                          â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:36 IST â”‚ 30 Nov 25 11:36 IST â”‚
â”‚ update-context â”‚                          â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:37 IST â”‚ 30 Nov 25 11:37 IST â”‚
â”‚ update-context â”‚                          â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:39 IST â”‚ 30 Nov 25 11:39 IST â”‚
â”‚ docker-env     â”‚ minikube docker-env      â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:39 IST â”‚ 30 Nov 25 11:39 IST â”‚
â”‚ docker-env     â”‚ minikube docker-env      â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:40 IST â”‚ 30 Nov 25 11:40 IST â”‚
â”‚ docker-env     â”‚ minikube docker-env      â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:40 IST â”‚ 30 Nov 25 11:40 IST â”‚
â”‚ docker-env     â”‚ minikube docker-env      â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:51 IST â”‚ 30 Nov 25 11:51 IST â”‚
â”‚ docker-env     â”‚ minikube docker-env      â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:54 IST â”‚ 30 Nov 25 11:54 IST â”‚
â”‚ service        â”‚ device-monitor-svc --url â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:56 IST â”‚ 30 Nov 25 11:56 IST â”‚
â”‚ ip             â”‚                          â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:56 IST â”‚ 30 Nov 25 11:56 IST â”‚
â”‚ ip             â”‚                          â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 11:56 IST â”‚ 30 Nov 25 11:56 IST â”‚
â”‚ docker-env     â”‚ minikube docker-env      â”‚ minikube â”‚ shaw â”‚ v1.37.0 â”‚ 30 Nov 25 12:24 IST â”‚ 30 Nov 25 12:24 IST â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


==> Last Start <==
Log file created at: 2025/11/30 11:25:22
Running on machine: tyyy
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1130 11:25:22.179421   31560 out.go:360] Setting OutFile to fd 1 ...
I1130 11:25:22.179713   31560 out.go:413] isatty.IsTerminal(1) = true
I1130 11:25:22.179718   31560 out.go:374] Setting ErrFile to fd 2...
I1130 11:25:22.179725   31560 out.go:413] isatty.IsTerminal(2) = true
I1130 11:25:22.179986   31560 root.go:338] Updating PATH: /home/shaw/.minikube/bin
W1130 11:25:22.180137   31560 root.go:314] Error reading config file at /home/shaw/.minikube/config/config.json: open /home/shaw/.minikube/config/config.json: no such file or directory
I1130 11:25:22.180733   31560 out.go:368] Setting JSON to false
I1130 11:25:22.183212   31560 start.go:130] hostinfo: {"hostname":"tyyy","uptime":2053,"bootTime":1764480069,"procs":394,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.14.0-35-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"16351f4c-4b0e-474e-8f25-ae3ec4bf208f"}
I1130 11:25:22.183326   31560 start.go:140] virtualization: kvm host
I1130 11:25:22.184804   31560 out.go:179] ðŸ˜„  minikube v1.37.0 on Ubuntu 24.04
W1130 11:25:22.185759   31560 preload.go:293] Failed to list preload files: open /home/shaw/.minikube/cache/preloaded-tarball: no such file or directory
I1130 11:25:22.185785   31560 notify.go:220] Checking for updates...
I1130 11:25:22.186049   31560 driver.go:421] Setting default libvirt URI to qemu:///system
I1130 11:25:22.218847   31560 docker.go:123] docker version: linux-29.0.4:Docker Engine - Community
I1130 11:25:22.219036   31560 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1130 11:25:22.290650   31560 info.go:266] docker info: {ID:5b4df5ef-1734-4434-bd3d-d4f9b03cd690 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:28 OomKillDisable:false NGoroutines:53 SystemTime:2025-11-30 11:25:22.277296412 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-35-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16438796288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:tyyy Labels:[] ExperimentalBuild:false ServerVersion:29.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.30.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3]] Warnings:<nil>}}
I1130 11:25:22.290807   31560 docker.go:318] overlay module found
I1130 11:25:22.291683   31560 out.go:179] âœ¨  Using the docker driver based on user configuration
I1130 11:25:22.292206   31560 start.go:304] selected driver: docker
I1130 11:25:22.292216   31560 start.go:918] validating driver "docker" against <nil>
I1130 11:25:22.292231   31560 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1130 11:25:22.292350   31560 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1130 11:25:22.366574   31560 info.go:266] docker info: {ID:5b4df5ef-1734-4434-bd3d-d4f9b03cd690 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:28 OomKillDisable:false NGoroutines:53 SystemTime:2025-11-30 11:25:22.35292833 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-35-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16438796288 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:tyyy Labels:[] ExperimentalBuild:false ServerVersion:29.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.30.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3]] Warnings:<nil>}}
I1130 11:25:22.366783   31560 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1130 11:25:22.368449   31560 start_flags.go:410] Using suggested 3900MB memory alloc based on sys=15677MB, container=15677MB
I1130 11:25:22.368667   31560 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1130 11:25:22.369515   31560 out.go:179] ðŸ“Œ  Using Docker driver with root privileges
I1130 11:25:22.370270   31560 cni.go:84] Creating CNI manager for ""
I1130 11:25:22.370345   31560 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 11:25:22.370355   31560 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1130 11:25:22.370438   31560 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1130 11:25:22.371593   31560 out.go:179] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I1130 11:25:22.372331   31560 cache.go:123] Beginning downloading kic base image for docker with docker
I1130 11:25:22.373060   31560 out.go:179] ðŸšœ  Pulling base image v0.0.48 ...
I1130 11:25:22.373607   31560 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 11:25:22.373747   31560 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1130 11:25:22.492685   31560 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1130 11:25:22.492839   31560 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1130 11:25:22.492990   31560 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1130 11:25:22.892832   31560 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1130 11:25:22.892850   31560 cache.go:58] Caching tarball of preloaded images
I1130 11:25:22.893004   31560 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 11:25:22.895681   31560 out.go:179] ðŸ’¾  Downloading Kubernetes v1.34.0 preload ...
I1130 11:25:22.896364   31560 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1130 11:25:23.638241   31560 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/shaw/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1130 11:33:26.960688   31560 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1130 11:33:26.960772   31560 preload.go:254] verifying checksum of /home/shaw/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1130 11:33:27.983306   31560 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1130 11:33:27.983770   31560 profile.go:143] Saving config to /home/shaw/.minikube/profiles/minikube/config.json ...
I1130 11:33:27.983791   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/profiles/minikube/config.json: {Name:mk11608a1dbaa1e9702a81c2f09f365fba9ccc3b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:07.156933   31560 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1130 11:34:07.156956   31560 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1130 11:34:12.055743   31560 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1130 11:34:12.055796   31560 cache.go:232] Successfully downloaded all kic artifacts
I1130 11:34:12.055841   31560 start.go:360] acquireMachinesLock for minikube: {Name:mke5bed646ccaad656afdd3803950c0a1f19098e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1130 11:34:12.055958   31560 start.go:364] duration metric: took 93.286Âµs to acquireMachinesLock for "minikube"
I1130 11:34:12.056003   31560 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1130 11:34:12.056092   31560 start.go:125] createHost starting for "" (driver="docker")
I1130 11:34:12.057062   31560 out.go:252] ðŸ”¥  Creating docker container (CPUs=2, Memory=3900MB) ...
I1130 11:34:12.057413   31560 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1130 11:34:12.057446   31560 client.go:168] LocalClient.Create starting
I1130 11:34:12.057635   31560 main.go:141] libmachine: Creating CA: /home/shaw/.minikube/certs/ca.pem
I1130 11:34:12.452301   31560 main.go:141] libmachine: Creating client certificate: /home/shaw/.minikube/certs/cert.pem
I1130 11:34:12.553181   31560 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1130 11:34:12.570513   31560 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1130 11:34:12.570571   31560 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1130 11:34:12.570583   31560 cli_runner.go:164] Run: docker network inspect minikube
W1130 11:34:12.591318   31560 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1130 11:34:12.591342   31560 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1130 11:34:12.591353   31560 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1130 11:34:12.591445   31560 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1130 11:34:12.613399   31560 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc00173c100}
I1130 11:34:12.613430   31560 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1130 11:34:12.613480   31560 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1130 11:34:12.689839   31560 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1130 11:34:12.689889   31560 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1130 11:34:12.689965   31560 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1130 11:34:12.733202   31560 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1130 11:34:12.757760   31560 oci.go:103] Successfully created a docker volume minikube
I1130 11:34:12.757845   31560 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1130 11:34:13.391630   31560 oci.go:107] Successfully prepared a docker volume minikube
I1130 11:34:13.391673   31560 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 11:34:13.391697   31560 kic.go:194] Starting extracting preloaded images to volume ...
I1130 11:34:13.391765   31560 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/shaw/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1130 11:34:15.451465   31560 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/shaw/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (2.059655024s)
I1130 11:34:15.451492   31560 kic.go:203] duration metric: took 2.05979037s to extract preloaded images to volume ...
W1130 11:34:15.451607   31560 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1130 11:34:15.451637   31560 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1130 11:34:15.451684   31560 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1130 11:34:15.523146   31560 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1130 11:34:15.969684   31560 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1130 11:34:15.994798   31560 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 11:34:16.020231   31560 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1130 11:34:16.097089   31560 oci.go:144] the created container "minikube" has a running status.
I1130 11:34:16.097119   31560 kic.go:225] Creating ssh key for kic: /home/shaw/.minikube/machines/minikube/id_rsa...
I1130 11:34:16.464324   31560 kic_runner.go:191] docker (temp): /home/shaw/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1130 11:34:16.489082   31560 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 11:34:16.508177   31560 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1130 11:34:16.508190   31560 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1130 11:34:16.569735   31560 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 11:34:16.597486   31560 machine.go:93] provisionDockerMachine start ...
I1130 11:34:16.597598   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:16.624397   31560 main.go:141] libmachine: Using SSH client type: native
I1130 11:34:16.624775   31560 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I1130 11:34:16.624790   31560 main.go:141] libmachine: About to run SSH command:
hostname
I1130 11:34:16.782825   31560 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1130 11:34:16.782846   31560 ubuntu.go:182] provisioning hostname "minikube"
I1130 11:34:16.782962   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:16.806890   31560 main.go:141] libmachine: Using SSH client type: native
I1130 11:34:16.807306   31560 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I1130 11:34:16.807319   31560 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1130 11:34:16.991482   31560 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1130 11:34:16.991580   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:17.013914   31560 main.go:141] libmachine: Using SSH client type: native
I1130 11:34:17.014325   31560 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I1130 11:34:17.014347   31560 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1130 11:34:17.176528   31560 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1130 11:34:17.176551   31560 ubuntu.go:188] set auth options {CertDir:/home/shaw/.minikube CaCertPath:/home/shaw/.minikube/certs/ca.pem CaPrivateKeyPath:/home/shaw/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/shaw/.minikube/machines/server.pem ServerKeyPath:/home/shaw/.minikube/machines/server-key.pem ClientKeyPath:/home/shaw/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/shaw/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/shaw/.minikube}
I1130 11:34:17.176694   31560 ubuntu.go:190] setting up certificates
I1130 11:34:17.176708   31560 provision.go:84] configureAuth start
I1130 11:34:17.176822   31560 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 11:34:17.199673   31560 provision.go:143] copyHostCerts
I1130 11:34:17.199763   31560 exec_runner.go:151] cp: /home/shaw/.minikube/certs/ca.pem --> /home/shaw/.minikube/ca.pem (1074 bytes)
I1130 11:34:17.199957   31560 exec_runner.go:151] cp: /home/shaw/.minikube/certs/cert.pem --> /home/shaw/.minikube/cert.pem (1115 bytes)
I1130 11:34:17.200055   31560 exec_runner.go:151] cp: /home/shaw/.minikube/certs/key.pem --> /home/shaw/.minikube/key.pem (1679 bytes)
I1130 11:34:17.200132   31560 provision.go:117] generating server cert: /home/shaw/.minikube/machines/server.pem ca-key=/home/shaw/.minikube/certs/ca.pem private-key=/home/shaw/.minikube/certs/ca-key.pem org=shaw.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1130 11:34:17.413267   31560 provision.go:177] copyRemoteCerts
I1130 11:34:17.413312   31560 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1130 11:34:17.413345   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:17.431123   31560 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/shaw/.minikube/machines/minikube/id_rsa Username:docker}
I1130 11:34:17.544741   31560 ssh_runner.go:362] scp /home/shaw/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1130 11:34:17.592953   31560 ssh_runner.go:362] scp /home/shaw/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I1130 11:34:17.636953   31560 ssh_runner.go:362] scp /home/shaw/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1130 11:34:17.678550   31560 provision.go:87] duration metric: took 501.824371ms to configureAuth
I1130 11:34:17.678573   31560 ubuntu.go:206] setting minikube options for container-runtime
I1130 11:34:17.678814   31560 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1130 11:34:17.678893   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:17.705653   31560 main.go:141] libmachine: Using SSH client type: native
I1130 11:34:17.706207   31560 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I1130 11:34:17.706224   31560 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1130 11:34:17.886067   31560 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1130 11:34:17.886094   31560 ubuntu.go:71] root file system type: overlay
I1130 11:34:17.886294   31560 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1130 11:34:17.886460   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:17.912064   31560 main.go:141] libmachine: Using SSH client type: native
I1130 11:34:17.912473   31560 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I1130 11:34:17.912594   31560 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1130 11:34:18.106121   31560 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1130 11:34:18.106257   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:18.132671   31560 main.go:141] libmachine: Using SSH client type: native
I1130 11:34:18.133112   31560 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I1130 11:34:18.133139   31560 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1130 11:34:19.842658   31560 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-11-30 06:04:18.101962598 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1130 11:34:19.842678   31560 machine.go:96] duration metric: took 3.245176927s to provisionDockerMachine
I1130 11:34:19.842689   31560 client.go:171] duration metric: took 7.785236563s to LocalClient.Create
I1130 11:34:19.842704   31560 start.go:167] duration metric: took 7.785293378s to libmachine.API.Create "minikube"
I1130 11:34:19.842710   31560 start.go:293] postStartSetup for "minikube" (driver="docker")
I1130 11:34:19.842726   31560 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1130 11:34:19.842782   31560 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1130 11:34:19.842821   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:19.866040   31560 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/shaw/.minikube/machines/minikube/id_rsa Username:docker}
I1130 11:34:19.998887   31560 ssh_runner.go:195] Run: cat /etc/os-release
I1130 11:34:20.005721   31560 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1130 11:34:20.005749   31560 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1130 11:34:20.005759   31560 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1130 11:34:20.005767   31560 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1130 11:34:20.005780   31560 filesync.go:126] Scanning /home/shaw/.minikube/addons for local assets ...
I1130 11:34:20.005872   31560 filesync.go:126] Scanning /home/shaw/.minikube/files for local assets ...
I1130 11:34:20.005903   31560 start.go:296] duration metric: took 163.181155ms for postStartSetup
I1130 11:34:20.006312   31560 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 11:34:20.032777   31560 profile.go:143] Saving config to /home/shaw/.minikube/profiles/minikube/config.json ...
I1130 11:34:20.033140   31560 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1130 11:34:20.033193   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:20.061613   31560 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/shaw/.minikube/machines/minikube/id_rsa Username:docker}
I1130 11:34:20.189026   31560 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1130 11:34:20.197968   31560 start.go:128] duration metric: took 8.141859876s to createHost
I1130 11:34:20.197990   31560 start.go:83] releasing machines lock for "minikube", held for 8.142017002s
I1130 11:34:20.198072   31560 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 11:34:20.224997   31560 ssh_runner.go:195] Run: cat /version.json
I1130 11:34:20.225071   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:20.225072   31560 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1130 11:34:20.225158   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:20.249997   31560 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/shaw/.minikube/machines/minikube/id_rsa Username:docker}
I1130 11:34:20.250729   31560 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/shaw/.minikube/machines/minikube/id_rsa Username:docker}
I1130 11:34:21.279225   31560 ssh_runner.go:235] Completed: cat /version.json: (1.054195778s)
I1130 11:34:21.279396   31560 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.054288632s)
I1130 11:34:21.279457   31560 ssh_runner.go:195] Run: systemctl --version
I1130 11:34:21.286591   31560 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1130 11:34:21.293447   31560 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1130 11:34:21.337641   31560 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1130 11:34:21.337762   31560 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1130 11:34:21.378473   31560 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1130 11:34:21.378495   31560 start.go:495] detecting cgroup driver to use...
I1130 11:34:21.378534   31560 detect.go:190] detected "systemd" cgroup driver on host os
I1130 11:34:21.378674   31560 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1130 11:34:21.403167   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1130 11:34:21.419998   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1130 11:34:21.435872   31560 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1130 11:34:21.435937   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1130 11:34:21.452739   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1130 11:34:21.470462   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1130 11:34:21.488088   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1130 11:34:21.504481   31560 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1130 11:34:21.518638   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1130 11:34:21.534726   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1130 11:34:21.549917   31560 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1130 11:34:21.563669   31560 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1130 11:34:21.575628   31560 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1130 11:34:21.588112   31560 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 11:34:21.673008   31560 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1130 11:34:21.795477   31560 start.go:495] detecting cgroup driver to use...
I1130 11:34:21.795528   31560 detect.go:190] detected "systemd" cgroup driver on host os
I1130 11:34:21.795631   31560 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1130 11:34:21.812308   31560 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1130 11:34:21.830631   31560 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1130 11:34:21.856667   31560 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1130 11:34:21.873514   31560 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1130 11:34:21.893196   31560 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1130 11:34:21.919788   31560 ssh_runner.go:195] Run: which cri-dockerd
I1130 11:34:21.924987   31560 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1130 11:34:21.939332   31560 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1130 11:34:21.964931   31560 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1130 11:34:22.049059   31560 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1130 11:34:22.129924   31560 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1130 11:34:22.130066   31560 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1130 11:34:22.156171   31560 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1130 11:34:22.171171   31560 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 11:34:22.252983   31560 ssh_runner.go:195] Run: sudo systemctl restart docker
I1130 11:34:23.493784   31560 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.240729508s)
I1130 11:34:23.493884   31560 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1130 11:34:23.514019   31560 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1130 11:34:23.535399   31560 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1130 11:34:23.553881   31560 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1130 11:34:23.645783   31560 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1130 11:34:23.732350   31560 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 11:34:23.812129   31560 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1130 11:34:23.853185   31560 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1130 11:34:23.872004   31560 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 11:34:23.961218   31560 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1130 11:34:24.138991   31560 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1130 11:34:24.158522   31560 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1130 11:34:24.158703   31560 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1130 11:34:24.164557   31560 start.go:563] Will wait 60s for crictl version
I1130 11:34:24.164606   31560 ssh_runner.go:195] Run: which crictl
I1130 11:34:24.170199   31560 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1130 11:34:24.267065   31560 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1130 11:34:24.267181   31560 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1130 11:34:24.303978   31560 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1130 11:34:24.340747   31560 out.go:252] ðŸ³  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1130 11:34:24.340907   31560 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1130 11:34:24.365506   31560 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1130 11:34:24.371758   31560 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1130 11:34:24.391265   31560 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1130 11:34:24.391384   31560 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1130 11:34:24.391436   31560 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1130 11:34:24.419420   31560 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1130 11:34:24.419440   31560 docker.go:621] Images already preloaded, skipping extraction
I1130 11:34:24.419523   31560 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1130 11:34:24.446687   31560 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1130 11:34:24.446706   31560 cache_images.go:85] Images are preloaded, skipping loading
I1130 11:34:24.446715   31560 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1130 11:34:24.446840   31560 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1130 11:34:24.447000   31560 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1130 11:34:24.550260   31560 cni.go:84] Creating CNI manager for ""
I1130 11:34:24.550284   31560 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 11:34:24.550297   31560 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1130 11:34:24.550325   31560 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1130 11:34:24.550465   31560 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1130 11:34:24.550531   31560 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1130 11:34:24.563293   31560 binaries.go:44] Found k8s binaries, skipping transfer
I1130 11:34:24.563364   31560 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1130 11:34:24.576574   31560 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1130 11:34:24.605907   31560 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1130 11:34:24.636066   31560 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1130 11:34:24.661569   31560 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1130 11:34:24.666482   31560 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1130 11:34:24.682911   31560 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 11:34:24.774253   31560 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1130 11:34:24.807785   31560 certs.go:68] Setting up /home/shaw/.minikube/profiles/minikube for IP: 192.168.49.2
I1130 11:34:24.807800   31560 certs.go:194] generating shared ca certs ...
I1130 11:34:24.807823   31560 certs.go:226] acquiring lock for ca certs: {Name:mk1505ce26ee602b089a7bd30f077bf9ff96729f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:24.807965   31560 certs.go:240] generating "minikubeCA" ca cert: /home/shaw/.minikube/ca.key
I1130 11:34:25.229016   31560 crypto.go:156] Writing cert to /home/shaw/.minikube/ca.crt ...
I1130 11:34:25.229031   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/ca.crt: {Name:mk5c86de84cf7c3d8ef87014940729e8e42a6006 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.229222   31560 crypto.go:164] Writing key to /home/shaw/.minikube/ca.key ...
I1130 11:34:25.229226   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/ca.key: {Name:mka583ccfe0dc76f28f99103e7aa3c9b58be923a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.229289   31560 certs.go:240] generating "proxyClientCA" ca cert: /home/shaw/.minikube/proxy-client-ca.key
I1130 11:34:25.294254   31560 crypto.go:156] Writing cert to /home/shaw/.minikube/proxy-client-ca.crt ...
I1130 11:34:25.294259   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/proxy-client-ca.crt: {Name:mk6ce6c749ecdac062a4f95a626932bad2b62f6b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.294327   31560 crypto.go:164] Writing key to /home/shaw/.minikube/proxy-client-ca.key ...
I1130 11:34:25.294330   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/proxy-client-ca.key: {Name:mkb56b9b55a784c4192bd96dfb7f201d790ce627 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.294373   31560 certs.go:256] generating profile certs ...
I1130 11:34:25.294426   31560 certs.go:363] generating signed profile cert for "minikube-user": /home/shaw/.minikube/profiles/minikube/client.key
I1130 11:34:25.294434   31560 crypto.go:68] Generating cert /home/shaw/.minikube/profiles/minikube/client.crt with IP's: []
I1130 11:34:25.484033   31560 crypto.go:156] Writing cert to /home/shaw/.minikube/profiles/minikube/client.crt ...
I1130 11:34:25.484048   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/profiles/minikube/client.crt: {Name:mk01afdc06ddd1a3f4e02c3b56b2326db0d8d7d1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.484217   31560 crypto.go:164] Writing key to /home/shaw/.minikube/profiles/minikube/client.key ...
I1130 11:34:25.484221   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/profiles/minikube/client.key: {Name:mk87848adee13f28bf2bee498d9b98800ad7d1e0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.484280   31560 certs.go:363] generating signed profile cert for "minikube": /home/shaw/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1130 11:34:25.484294   31560 crypto.go:68] Generating cert /home/shaw/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1130 11:34:25.539347   31560 crypto.go:156] Writing cert to /home/shaw/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1130 11:34:25.539354   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk268447246187027d2630de8e3b725e9b99c2ec Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.539443   31560 crypto.go:164] Writing key to /home/shaw/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1130 11:34:25.539447   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk4bf1fed6bea651c6b9639a4046ab9f2130a65c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.539501   31560 certs.go:381] copying /home/shaw/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/shaw/.minikube/profiles/minikube/apiserver.crt
I1130 11:34:25.539568   31560 certs.go:385] copying /home/shaw/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/shaw/.minikube/profiles/minikube/apiserver.key
I1130 11:34:25.539602   31560 certs.go:363] generating signed profile cert for "aggregator": /home/shaw/.minikube/profiles/minikube/proxy-client.key
I1130 11:34:25.539613   31560 crypto.go:68] Generating cert /home/shaw/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1130 11:34:25.980525   31560 crypto.go:156] Writing cert to /home/shaw/.minikube/profiles/minikube/proxy-client.crt ...
I1130 11:34:25.980541   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/profiles/minikube/proxy-client.crt: {Name:mkb5627121141f9aa86206e1d0f1cbd2bdfd919f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.980713   31560 crypto.go:164] Writing key to /home/shaw/.minikube/profiles/minikube/proxy-client.key ...
I1130 11:34:25.980717   31560 lock.go:35] WriteFile acquiring /home/shaw/.minikube/profiles/minikube/proxy-client.key: {Name:mk8c146e8ef962fed34052e2424f12821fb49cbd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:25.980855   31560 certs.go:484] found cert: /home/shaw/.minikube/certs/ca-key.pem (1679 bytes)
I1130 11:34:25.980891   31560 certs.go:484] found cert: /home/shaw/.minikube/certs/ca.pem (1074 bytes)
I1130 11:34:25.980912   31560 certs.go:484] found cert: /home/shaw/.minikube/certs/cert.pem (1115 bytes)
I1130 11:34:25.980928   31560 certs.go:484] found cert: /home/shaw/.minikube/certs/key.pem (1679 bytes)
I1130 11:34:25.981497   31560 ssh_runner.go:362] scp /home/shaw/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1130 11:34:26.015285   31560 ssh_runner.go:362] scp /home/shaw/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1130 11:34:26.050977   31560 ssh_runner.go:362] scp /home/shaw/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1130 11:34:26.082901   31560 ssh_runner.go:362] scp /home/shaw/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1130 11:34:26.116394   31560 ssh_runner.go:362] scp /home/shaw/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1130 11:34:26.153764   31560 ssh_runner.go:362] scp /home/shaw/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1130 11:34:26.191317   31560 ssh_runner.go:362] scp /home/shaw/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1130 11:34:26.226006   31560 ssh_runner.go:362] scp /home/shaw/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1130 11:34:26.262873   31560 ssh_runner.go:362] scp /home/shaw/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1130 11:34:26.303102   31560 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1130 11:34:26.332938   31560 ssh_runner.go:195] Run: openssl version
I1130 11:34:26.342566   31560 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1130 11:34:26.358758   31560 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1130 11:34:26.364623   31560 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 30 06:04 /usr/share/ca-certificates/minikubeCA.pem
I1130 11:34:26.364676   31560 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1130 11:34:26.375634   31560 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1130 11:34:26.390493   31560 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1130 11:34:26.396342   31560 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1130 11:34:26.396398   31560 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1130 11:34:26.396543   31560 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1130 11:34:26.425829   31560 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1130 11:34:26.439539   31560 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1130 11:34:26.453899   31560 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1130 11:34:26.454009   31560 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1130 11:34:26.468913   31560 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1130 11:34:26.468936   31560 kubeadm.go:157] found existing configuration files:

I1130 11:34:26.468999   31560 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1130 11:34:26.484488   31560 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1130 11:34:26.484583   31560 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1130 11:34:26.498019   31560 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1130 11:34:26.510873   31560 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1130 11:34:26.510953   31560 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1130 11:34:26.524589   31560 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1130 11:34:26.538390   31560 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1130 11:34:26.538452   31560 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1130 11:34:26.551405   31560 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1130 11:34:26.563450   31560 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1130 11:34:26.563504   31560 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1130 11:34:26.577147   31560 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1130 11:34:26.643548   31560 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1130 11:34:26.647399   31560 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.14.0-35-generic\n", err: exit status 1
I1130 11:34:26.721153   31560 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1130 11:34:37.197629   31560 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1130 11:34:37.197699   31560 kubeadm.go:310] [preflight] Running pre-flight checks
I1130 11:34:37.197818   31560 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I1130 11:34:37.197910   31560 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.14.0-35-generic[0m
I1130 11:34:37.197956   31560 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I1130 11:34:37.198081   31560 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I1130 11:34:37.198170   31560 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I1130 11:34:37.198259   31560 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I1130 11:34:37.198339   31560 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I1130 11:34:37.198399   31560 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I1130 11:34:37.198457   31560 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I1130 11:34:37.198552   31560 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I1130 11:34:37.198627   31560 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I1130 11:34:37.198732   31560 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1130 11:34:37.198889   31560 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1130 11:34:37.199022   31560 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1130 11:34:37.199108   31560 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1130 11:34:37.199943   31560 out.go:252]     â–ª Generating certificates and keys ...
I1130 11:34:37.200074   31560 kubeadm.go:310] [certs] Using existing ca certificate authority
I1130 11:34:37.200215   31560 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1130 11:34:37.200328   31560 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1130 11:34:37.200421   31560 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1130 11:34:37.200543   31560 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1130 11:34:37.200616   31560 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1130 11:34:37.200712   31560 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1130 11:34:37.200937   31560 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1130 11:34:37.201017   31560 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1130 11:34:37.201186   31560 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1130 11:34:37.201284   31560 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1130 11:34:37.201392   31560 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1130 11:34:37.201456   31560 kubeadm.go:310] [certs] Generating "sa" key and public key
I1130 11:34:37.201536   31560 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1130 11:34:37.201622   31560 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1130 11:34:37.201709   31560 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1130 11:34:37.201795   31560 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1130 11:34:37.201921   31560 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1130 11:34:37.202015   31560 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1130 11:34:37.202156   31560 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1130 11:34:37.202264   31560 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1130 11:34:37.202969   31560 out.go:252]     â–ª Booting up control plane ...
I1130 11:34:37.203218   31560 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1130 11:34:37.203325   31560 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1130 11:34:37.203449   31560 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1130 11:34:37.203614   31560 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1130 11:34:37.203750   31560 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1130 11:34:37.203931   31560 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1130 11:34:37.204063   31560 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1130 11:34:37.204118   31560 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1130 11:34:37.204321   31560 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1130 11:34:37.204489   31560 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1130 11:34:37.204582   31560 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.001572988s
I1130 11:34:37.204715   31560 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1130 11:34:37.204854   31560 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1130 11:34:37.205004   31560 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1130 11:34:37.205127   31560 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1130 11:34:37.205235   31560 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 2.007186144s
I1130 11:34:37.205351   31560 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 2.684507177s
I1130 11:34:37.205442   31560 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 4.502197036s
I1130 11:34:37.205600   31560 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1130 11:34:37.205792   31560 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1130 11:34:37.205900   31560 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1130 11:34:37.206197   31560 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1130 11:34:37.206286   31560 kubeadm.go:310] [bootstrap-token] Using token: wqpjmf.gwmfr7ojr51pimya
I1130 11:34:37.207215   31560 out.go:252]     â–ª Configuring RBAC rules ...
I1130 11:34:37.207368   31560 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1130 11:34:37.207498   31560 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1130 11:34:37.207700   31560 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1130 11:34:37.207924   31560 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1130 11:34:37.208117   31560 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1130 11:34:37.208251   31560 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1130 11:34:37.208428   31560 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1130 11:34:37.208491   31560 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1130 11:34:37.208561   31560 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1130 11:34:37.208569   31560 kubeadm.go:310] 
I1130 11:34:37.208664   31560 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1130 11:34:37.208669   31560 kubeadm.go:310] 
I1130 11:34:37.208781   31560 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1130 11:34:37.208785   31560 kubeadm.go:310] 
I1130 11:34:37.208826   31560 kubeadm.go:310]   mkdir -p $HOME/.kube
I1130 11:34:37.208934   31560 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1130 11:34:37.209006   31560 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1130 11:34:37.209010   31560 kubeadm.go:310] 
I1130 11:34:37.209085   31560 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1130 11:34:37.209089   31560 kubeadm.go:310] 
I1130 11:34:37.209153   31560 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1130 11:34:37.209157   31560 kubeadm.go:310] 
I1130 11:34:37.209228   31560 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1130 11:34:37.209333   31560 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1130 11:34:37.209451   31560 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1130 11:34:37.209456   31560 kubeadm.go:310] 
I1130 11:34:37.209582   31560 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1130 11:34:37.209708   31560 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1130 11:34:37.209713   31560 kubeadm.go:310] 
I1130 11:34:37.209855   31560 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token wqpjmf.gwmfr7ojr51pimya \
I1130 11:34:37.210037   31560 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:91498f1f70f361490f73ddaae8b38e16dcfee8f45d4ee099aae1d31b241378ab \
I1130 11:34:37.210067   31560 kubeadm.go:310] 	--control-plane 
I1130 11:34:37.210073   31560 kubeadm.go:310] 
I1130 11:34:37.210200   31560 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1130 11:34:37.210205   31560 kubeadm.go:310] 
I1130 11:34:37.210331   31560 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token wqpjmf.gwmfr7ojr51pimya \
I1130 11:34:37.210505   31560 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:91498f1f70f361490f73ddaae8b38e16dcfee8f45d4ee099aae1d31b241378ab 
I1130 11:34:37.210515   31560 cni.go:84] Creating CNI manager for ""
I1130 11:34:37.210532   31560 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 11:34:37.211336   31560 out.go:179] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I1130 11:34:37.211940   31560 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1130 11:34:37.226491   31560 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1130 11:34:37.253679   31560 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1130 11:34:37.253759   31560 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1130 11:34:37.253769   31560 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_11_30T11_34_37_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1130 11:34:37.265705   31560 ops.go:34] apiserver oom_adj: -16
I1130 11:34:37.347405   31560 kubeadm.go:1105] duration metric: took 93.715733ms to wait for elevateKubeSystemPrivileges
I1130 11:34:37.367231   31560 kubeadm.go:394] duration metric: took 10.970827026s to StartCluster
I1130 11:34:37.367270   31560 settings.go:142] acquiring lock: {Name:mk396870e12caac40120aa88888b620797b519ec Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:37.367359   31560 settings.go:150] Updating kubeconfig:  /home/shaw/.kube/config
I1130 11:34:37.368137   31560 lock.go:35] WriteFile acquiring /home/shaw/.kube/config: {Name:mk6b4c26963761c99739d924d1d622dc30c68935 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 11:34:37.368375   31560 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1130 11:34:37.368397   31560 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1130 11:34:37.368461   31560 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1130 11:34:37.368586   31560 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1130 11:34:37.368627   31560 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1130 11:34:37.368653   31560 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1130 11:34:37.368657   31560 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1130 11:34:37.368664   31560 host.go:66] Checking if "minikube" exists ...
I1130 11:34:37.368683   31560 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1130 11:34:37.369022   31560 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I1130 11:34:37.369208   31560 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 11:34:37.369556   31560 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 11:34:37.370023   31560 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 11:34:37.395618   31560 out.go:179]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1130 11:34:37.396549   31560 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1130 11:34:37.396565   31560 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1130 11:34:37.396651   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:37.397355   31560 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1130 11:34:37.397403   31560 host.go:66] Checking if "minikube" exists ...
I1130 11:34:37.398071   31560 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 11:34:37.425034   31560 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/shaw/.minikube/machines/minikube/id_rsa Username:docker}
I1130 11:34:37.427078   31560 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1130 11:34:37.427095   31560 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1130 11:34:37.427170   31560 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 11:34:37.451499   31560 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/shaw/.minikube/machines/minikube/id_rsa Username:docker}
I1130 11:34:37.476554   31560 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1130 11:34:37.512887   31560 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1130 11:34:37.548271   31560 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1130 11:34:37.571692   31560 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1130 11:34:37.664918   31560 start.go:976] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1130 11:34:37.666806   31560 api_server.go:52] waiting for apiserver process to appear ...
I1130 11:34:37.666882   31560 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1130 11:34:37.869453   31560 api_server.go:72] duration metric: took 501.025594ms to wait for apiserver process to appear ...
I1130 11:34:37.869470   31560 api_server.go:88] waiting for apiserver healthz status ...
I1130 11:34:37.869497   31560 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1130 11:34:37.875252   31560 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1130 11:34:37.876157   31560 api_server.go:141] control plane version: v1.34.0
I1130 11:34:37.876177   31560 api_server.go:131] duration metric: took 6.699623ms to wait for apiserver health ...
I1130 11:34:37.876184   31560 system_pods.go:43] waiting for kube-system pods to appear ...
I1130 11:34:37.877032   31560 out.go:179] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I1130 11:34:37.877897   31560 addons.go:514] duration metric: took 509.448203ms for enable addons: enabled=[storage-provisioner default-storageclass]
I1130 11:34:37.879190   31560 system_pods.go:59] 5 kube-system pods found
I1130 11:34:37.879210   31560 system_pods.go:61] "etcd-minikube" [f9e01454-30bd-427f-a3ed-c394711cef8e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1130 11:34:37.879220   31560 system_pods.go:61] "kube-apiserver-minikube" [b2b7a40f-5624-4b58-8347-0d5f82670dce] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1130 11:34:37.879228   31560 system_pods.go:61] "kube-controller-manager-minikube" [76e5fd0e-8120-45ff-af1f-a517ec1ad2a9] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1130 11:34:37.879234   31560 system_pods.go:61] "kube-scheduler-minikube" [62b87595-cc5c-4322-a24d-1aee8178da5b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1130 11:34:37.879237   31560 system_pods.go:61] "storage-provisioner" [3c24edea-cbc3-4f8a-b922-7a19884b9a75] Pending
I1130 11:34:37.879244   31560 system_pods.go:74] duration metric: took 3.053364ms to wait for pod list to return data ...
I1130 11:34:37.879255   31560 kubeadm.go:578] duration metric: took 510.831692ms to wait for: map[apiserver:true system_pods:true]
I1130 11:34:37.879268   31560 node_conditions.go:102] verifying NodePressure condition ...
I1130 11:34:37.881482   31560 node_conditions.go:122] node storage ephemeral capacity is 226059820Ki
I1130 11:34:37.881498   31560 node_conditions.go:123] node cpu capacity is 16
I1130 11:34:37.881509   31560 node_conditions.go:105] duration metric: took 2.237694ms to run NodePressure ...
I1130 11:34:37.881523   31560 start.go:241] waiting for startup goroutines ...
I1130 11:34:38.169840   31560 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1130 11:34:38.169895   31560 start.go:246] waiting for cluster config update ...
I1130 11:34:38.169912   31560 start.go:255] writing updated cluster config ...
I1130 11:34:38.170284   31560 ssh_runner.go:195] Run: rm -f paused
I1130 11:34:38.250591   31560 start.go:617] kubectl: 1.31.0, cluster: 1.34.0 (minor skew: 3)
I1130 11:34:38.251465   31560 out.go:203] 
W1130 11:34:38.252022   31560 out.go:285] â—  /usr/local/bin/kubectl is version 1.31.0, which may have incompatibilities with Kubernetes 1.34.0.
I1130 11:34:38.252517   31560 out.go:179]     â–ª Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I1130 11:34:38.253348   31560 out.go:179] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 30 06:10:36 minikube dockerd[1146]: time="2025-11-30T06:10:36.115782204Z" level=error msg=/moby.buildkit.v1.Control/Solve error="rpc error: code = Unknown desc = failed to read dockerfile: open Dockerfile: no such file or directory" spanID=06067198dea05ffd traceID=bdaef32007ea37525ee6e467fb06e249
Nov 30 06:10:40 minikube dockerd[1146]: time="2025-11-30T06:10:40.832845261Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 30 06:10:40 minikube dockerd[1146]: time="2025-11-30T06:10:40.832944538Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 30 06:10:51 minikube dockerd[1146]: time="2025-11-30T06:10:51.968203754Z" level=error msg=/moby.buildkit.v1.frontend.LLBBridge/Solve error="rpc error: code = Unknown desc = failed to read dockerfile: open Dockerfile: no such file or directory" spanID=40ad906d682fadbb traceID=25b7b42224d4f64e9216a627041ff6f2
Nov 30 06:10:51 minikube dockerd[1146]: time="2025-11-30T06:10:51.985531416Z" level=error msg=/moby.buildkit.v1.Control/Solve error="rpc error: code = Unknown desc = failed to read dockerfile: open Dockerfile: no such file or directory" spanID=9130d14684bbe919 traceID=25b7b42224d4f64e9216a627041ff6f2
Nov 30 06:11:19 minikube dockerd[1146]: time="2025-11-30T06:11:19.479987392Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 30 06:11:19 minikube dockerd[1146]: time="2025-11-30T06:11:19.480059378Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 30 06:13:38 minikube dockerd[1146]: time="2025-11-30T06:13:38.707507663Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 30 06:13:38 minikube dockerd[1146]: time="2025-11-30T06:13:38.707681796Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 30 06:14:16 minikube dockerd[1146]: time="2025-11-30T06:14:16.633389541Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Nov 30 06:14:16 minikube dockerd[1146]: time="2025-11-30T06:14:16.633439963Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Nov 30 06:22:11 minikube cri-dockerd[1459]: time="2025-11-30T06:22:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aafb757b44ef0fbcadfd47261e6b28d8b8f5d1c6b35ed11171895cb8a283ad76/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 06:22:18 minikube dockerd[1146]: time="2025-11-30T06:22:18.958717575Z" level=info msg="ignoring event" container=9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:22:19 minikube dockerd[1146]: time="2025-11-30T06:22:19.174861464Z" level=info msg="ignoring event" container=37f9a7d848541137e78e15ed475759ebedb7f0e990fafffc9ef0057913228022 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:22:19 minikube cri-dockerd[1459]: time="2025-11-30T06:22:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/50a0003ac10bdee6f70e581f06a4b72e381117a8c1d928f908e2defedc1d2c0e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 06:22:25 minikube dockerd[1146]: time="2025-11-30T06:22:25.020058319Z" level=info msg="ignoring event" container=d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:22:25 minikube dockerd[1146]: time="2025-11-30T06:22:25.245421370Z" level=info msg="ignoring event" container=3feb99dfc9d7394deff6d28a06106045901d459514fc80f962e2ac5c2c1f4e79 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:22:27 minikube cri-dockerd[1459]: time="2025-11-30T06:22:27Z" level=error msg="error getting RW layer size for container ID 'd09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850': Error response from daemon: No such container: d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850"
Nov 30 06:22:27 minikube cri-dockerd[1459]: time="2025-11-30T06:22:27Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'd09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850'"
Nov 30 06:49:55 minikube dockerd[1146]: time="2025-11-30T06:49:55.957260545Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:49:56 minikube dockerd[1146]: time="2025-11-30T06:49:56.740337194Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:49:57 minikube dockerd[1146]: time="2025-11-30T06:49:57.609220967Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:49:58 minikube dockerd[1146]: time="2025-11-30T06:49:58.598643406Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:50:00 minikube dockerd[1146]: time="2025-11-30T06:50:00.022359945Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:50:02 minikube dockerd[1146]: time="2025-11-30T06:50:02.241271616Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:50:03 minikube dockerd[1146]: time="2025-11-30T06:50:03.501528949Z" level=error msg="Error setting up exec command in container jenkins-server: Container f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 is restarting, wait until the container is running"
Nov 30 06:50:06 minikube dockerd[1146]: time="2025-11-30T06:50:06.028860618Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:50:13 minikube dockerd[1146]: time="2025-11-30T06:50:13.049449478Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:50:14 minikube dockerd[1146]: time="2025-11-30T06:50:14.926536248Z" level=error msg="Error setting up exec command in container jenkins-server: Container f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 is restarting, wait until the container is running"
Nov 30 06:50:16 minikube dockerd[1146]: time="2025-11-30T06:50:16.296071052Z" level=error msg="Error setting up exec command in container jenkins-server: Container f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 is restarting, wait until the container is running"
Nov 30 06:50:24 minikube dockerd[1146]: time="2025-11-30T06:50:24.301297637Z" level=error msg="Error setting up exec command in container jenkins-server: Container f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 is restarting, wait until the container is running"
Nov 30 06:50:26 minikube dockerd[1146]: time="2025-11-30T06:50:26.468062617Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:50:52 minikube dockerd[1146]: time="2025-11-30T06:50:52.647126261Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:51:44 minikube dockerd[1146]: time="2025-11-30T06:51:44.447435955Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:52:45 minikube dockerd[1146]: time="2025-11-30T06:52:45.062643652Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:53:45 minikube dockerd[1146]: time="2025-11-30T06:53:45.680251253Z" level=info msg="ignoring event" container=f108472f096828443db4df5cfba981c508eb649a346b89caf95f20d4953a5fc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:22 minikube dockerd[1146]: time="2025-11-30T06:54:22.046714058Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 937745c0564b5f9482bc82f87baeff61b1d2a6f1610cafcffc2a5e31aa3a6f3e], retrying...."
Nov 30 06:54:24 minikube dockerd[1146]: time="2025-11-30T06:54:24.842212407Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:25 minikube dockerd[1146]: time="2025-11-30T06:54:25.642131141Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:26 minikube dockerd[1146]: time="2025-11-30T06:54:26.467639782Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:27 minikube dockerd[1146]: time="2025-11-30T06:54:27.519284403Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:27 minikube dockerd[1146]: time="2025-11-30T06:54:27.738340216Z" level=error msg="Error setting up exec command in container jenkins-server: Container 68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 is restarting, wait until the container is running"
Nov 30 06:54:28 minikube dockerd[1146]: time="2025-11-30T06:54:28.956245950Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:31 minikube dockerd[1146]: time="2025-11-30T06:54:31.157761110Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:34 minikube dockerd[1146]: time="2025-11-30T06:54:34.957591827Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:41 minikube dockerd[1146]: time="2025-11-30T06:54:41.983146023Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:54:55 minikube dockerd[1146]: time="2025-11-30T06:54:55.444385863Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:55:21 minikube dockerd[1146]: time="2025-11-30T06:55:21.642344849Z" level=info msg="ignoring event" container=68b5525c5e94c20f2cee85c158068b02ec65f8d3b6fe820fa70ce8ffa5737864 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:55:58 minikube dockerd[1146]: time="2025-11-30T06:55:58.306982809Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 45250b6b1ad3a71d10fccd1e8570813839a3e6e7f727f8e007f8b9d7e4ed50e3], retrying...."
Nov 30 06:56:06 minikube dockerd[1146]: time="2025-11-30T06:56:06.626397522Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:07 minikube dockerd[1146]: time="2025-11-30T06:56:07.418911868Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:08 minikube dockerd[1146]: time="2025-11-30T06:56:08.201537617Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:09 minikube dockerd[1146]: time="2025-11-30T06:56:09.212471984Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:10 minikube dockerd[1146]: time="2025-11-30T06:56:10.589687555Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:12 minikube dockerd[1146]: time="2025-11-30T06:56:12.777826603Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:16 minikube dockerd[1146]: time="2025-11-30T06:56:16.564474962Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:23 minikube dockerd[1146]: time="2025-11-30T06:56:23.566315250Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:56:36 minikube dockerd[1146]: time="2025-11-30T06:56:36.985331478Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:57:03 minikube dockerd[1146]: time="2025-11-30T06:57:03.414674346Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 06:57:55 minikube dockerd[1146]: time="2025-11-30T06:57:55.346383129Z" level=info msg="ignoring event" container=5d1aa148394a23aefab8ceb7419aec4d636abcddeea1ee7a3d540d9649f34ea7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
478b53c3bdd42       bf8d4f107ac67       36 minutes ago      Running             device-monitor            0                   50a0003ac10bd       device-monitor-7d4bcfcccf-7wgfz
ae3a4f566bd41       bf8d4f107ac67       36 minutes ago      Running             device-monitor            0                   aafb757b44ef0       device-monitor-7d4bcfcccf-cbn9t
bb24e8b9a1302       6e38f40d628db       53 minutes ago      Running             storage-provisioner       0                   614b1669aaad6       storage-provisioner
ab249fc83eeb1       52546a367cc9e       53 minutes ago      Running             coredns                   0                   7e41669c29474       coredns-66bc5c9577-bsrc9
48b35875e6719       df0860106674d       53 minutes ago      Running             kube-proxy                0                   5667e5b1c65e3       kube-proxy-gjcjx
addbce052935d       46169d968e920       54 minutes ago      Running             kube-scheduler            0                   b08656c910b33       kube-scheduler-minikube
e2ce1af4bbd91       5f1f5298c888d       54 minutes ago      Running             etcd                      0                   961987aeef096       etcd-minikube
d9dd81284f258       a0af72f2ec6d6       54 minutes ago      Running             kube-controller-manager   0                   9d03afe7dd67e       kube-controller-manager-minikube
b2e09618dd81f       90550c43ad2bc       54 minutes ago      Running             kube-apiserver            0                   897705ef7237b       kube-apiserver-minikube


==> coredns [ab249fc83eeb] <==
maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:38283 - 4175 "HINFO IN 5637619105684218504.5634517378402787669. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.372460838s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_30T11_34_37_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 30 Nov 2025 06:04:34 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 30 Nov 2025 06:58:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 30 Nov 2025 06:57:09 +0000   Sun, 30 Nov 2025 06:04:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 30 Nov 2025 06:57:09 +0000   Sun, 30 Nov 2025 06:04:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 30 Nov 2025 06:57:09 +0000   Sun, 30 Nov 2025 06:04:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 30 Nov 2025 06:57:09 +0000   Sun, 30 Nov 2025 06:04:34 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  226059820Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16053512Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  226059820Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16053512Ki
  pods:               110
System Info:
  Machine ID:                 a086d42e225b4833a57748aa45b37dbb
  System UUID:                14454b03-d382-4e44-8ac3-ecfe57c55453
  Boot ID:                    34e48b04-0205-4446-a357-e68b6828a884
  Kernel Version:             6.14.0-35-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     device-monitor-7d4bcfcccf-7wgfz     0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m
  default                     device-monitor-7d4bcfcccf-cbn9t     0 (0%)        0 (0%)      0 (0%)           0 (0%)         36m
  kube-system                 coredns-66bc5c9577-bsrc9            100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     53m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         54m
  kube-system                 kube-apiserver-minikube             250m (1%)     0 (0%)      0 (0%)           0 (0%)         54m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         54m
  kube-system                 kube-proxy-gjcjx                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         54m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 53m   kube-proxy       
  Normal  Starting                 54m   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  54m   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  54m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    54m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     54m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           54m   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000004] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000005] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[ +19.029102] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000006] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000007] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[ +33.247184] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000003] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000003] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[Nov30 06:57] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000005] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000005] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[ +16.283550] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000008] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000008] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +8.696254] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000006] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000006] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[ +14.034930] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000006] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000006] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.122648] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000005] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000004] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.102514] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000005] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000005] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.341450] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000004] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000005] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.089444] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000004] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000004] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +4.097091] workqueue: delayed_fput hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +4.167478] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000004] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000004] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[Nov30 06:58] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000003] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000005] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +5.966638] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000003] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000002] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +7.301599] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000003] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000004] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +0.515952] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000009] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000009] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +1.959248] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000008] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000009] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +1.421703] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000004] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000004] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[  +1.335446] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000009] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000009] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)
[ +19.166875] rtw_8821ce 0000:02:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)
[  +0.000005] rtw_8821ce 0000:02:00.0:   device [10ec:c821] error status/mask=00000001/0000e000
[  +0.000006] rtw_8821ce 0000:02:00.0:    [ 0] RxErr                  (First)


==> etcd [e2ce1af4bbd9] <==
{"level":"warn","ts":"2025-11-30T06:04:33.196131Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44080","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.203803Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44100","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.210558Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44112","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.220596Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44136","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.227608Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44154","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.234274Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44182","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.240810Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44208","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.245930Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44224","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.256967Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44244","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.265548Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44256","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.273413Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44266","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.284072Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44294","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.290433Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44312","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.299733Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44328","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.307939Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44352","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.316521Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44358","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.322851Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44372","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.341196Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44398","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.353318Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44422","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.380609Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44440","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.392716Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44460","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.401732Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44480","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.410290Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44496","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.425162Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44516","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.436934Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44540","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.444903Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44554","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.454565Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44562","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.463391Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44564","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.487632Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44588","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.503373Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44608","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.514186Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44628","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.523089Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44636","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-30T06:04:33.568735Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:44650","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-11-30T06:14:32.972385Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":704}
{"level":"info","ts":"2025-11-30T06:14:33.003709Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":704,"took":"30.889671ms","hash":1931548005,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1814528,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-11-30T06:14:33.003807Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1931548005,"revision":704,"compact-revision":-1}
{"level":"info","ts":"2025-11-30T06:19:32.981057Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":1021}
{"level":"info","ts":"2025-11-30T06:19:32.999163Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":1021,"took":"17.451287ms","hash":779937985,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1200128,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-11-30T06:19:32.999215Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":779937985,"revision":1021,"compact-revision":704}
{"level":"info","ts":"2025-11-30T06:24:32.986239Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":1265}
{"level":"info","ts":"2025-11-30T06:24:32.989781Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":1265,"took":"3.149435ms","hash":769379713,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1257472,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-11-30T06:24:32.989831Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":769379713,"revision":1265,"compact-revision":1021}
{"level":"info","ts":"2025-11-30T06:29:32.993381Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":1586}
{"level":"info","ts":"2025-11-30T06:29:32.996150Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":1586,"took":"2.337259ms","hash":315882189,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1261568,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-11-30T06:29:32.996200Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":315882189,"revision":1586,"compact-revision":1265}
{"level":"info","ts":"2025-11-30T06:34:33.000633Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":1825}
{"level":"info","ts":"2025-11-30T06:34:33.016295Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":1825,"took":"13.797519ms","hash":1732228564,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":987136,"current-db-size-in-use":"987 kB"}
{"level":"info","ts":"2025-11-30T06:34:33.016455Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1732228564,"revision":1825,"compact-revision":1586}
{"level":"info","ts":"2025-11-30T06:39:33.007102Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":2065}
{"level":"info","ts":"2025-11-30T06:39:33.022434Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":2065,"took":"14.97384ms","hash":4186920052,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":983040,"current-db-size-in-use":"983 kB"}
{"level":"info","ts":"2025-11-30T06:39:33.022498Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4186920052,"revision":2065,"compact-revision":1825}
{"level":"info","ts":"2025-11-30T06:44:33.012155Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":2305}
{"level":"info","ts":"2025-11-30T06:44:33.023904Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":2305,"took":"10.42114ms","hash":83316223,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":983040,"current-db-size-in-use":"983 kB"}
{"level":"info","ts":"2025-11-30T06:44:33.024206Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":83316223,"revision":2305,"compact-revision":2065}
{"level":"info","ts":"2025-11-30T06:49:33.020531Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":2546}
{"level":"info","ts":"2025-11-30T06:49:33.023758Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":2546,"took":"2.646255ms","hash":3423692491,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":991232,"current-db-size-in-use":"991 kB"}
{"level":"info","ts":"2025-11-30T06:49:33.023817Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3423692491,"revision":2546,"compact-revision":2305}
{"level":"info","ts":"2025-11-30T06:54:33.023975Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":2787}
{"level":"info","ts":"2025-11-30T06:54:33.025791Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":2787,"took":"1.456255ms","hash":638761863,"current-db-size-bytes":1814528,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":983040,"current-db-size-in-use":"983 kB"}
{"level":"info","ts":"2025-11-30T06:54:33.025836Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":638761863,"revision":2787,"compact-revision":2546}


==> kernel <==
 06:58:41 up  1:37,  0 users,  load average: 0.93, 1.35, 1.37
Linux minikube 6.14.0-35-generic #35~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Oct 14 13:55:17 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [b2e09618dd81] <==
I1130 06:24:37.273709       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:24:51.381081       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:25:55.067286       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:26:00.361511       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:27:19.118522       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:27:23.945308       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:28:20.525301       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:28:35.467520       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:29:43.161521       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:29:47.427818       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:30:48.781484       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:31:17.253900       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:32:10.216258       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:32:33.290303       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:33:19.942103       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:33:45.440680       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:34:33.944425       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1130 06:34:46.514124       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:34:47.406714       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:35:47.494073       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:36:00.119555       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:37:04.598572       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:37:10.796611       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:38:09.764821       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:38:13.253861       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:39:24.078137       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:39:26.992463       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:40:26.396731       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:40:34.172624       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:41:41.350316       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:41:45.466120       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:42:44.784469       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:43:07.920015       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:43:58.985805       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:44:19.337213       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:44:33.946951       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1130 06:45:07.500127       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:45:28.769148       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:46:24.081413       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:46:46.108455       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:47:26.246571       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:48:15.542159       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:48:49.723173       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:49:19.990223       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:49:54.217168       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:50:23.572216       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:51:03.015157       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:51:44.173745       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:52:03.595236       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:53:03.206420       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:53:04.955847       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:54:19.072477       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:54:28.398652       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:54:33.946073       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1130 06:55:40.765180       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:55:50.931804       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:56:41.129973       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:57:16.474619       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:57:51.133664       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1130 06:58:35.905409       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [d9dd81284f25] <==
I1130 06:04:40.850527       1 controllermanager.go:781] "Started controller" controller="taint-eviction-controller"
I1130 06:04:40.850655       1 taint_eviction.go:282] "Starting" logger="taint-eviction-controller" controller="taint-eviction-controller"
I1130 06:04:40.850762       1 taint_eviction.go:288] "Sending events to api server" logger="taint-eviction-controller"
I1130 06:04:40.850848       1 shared_informer.go:349] "Waiting for caches to sync" controller="taint-eviction-controller"
I1130 06:04:40.859288       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1130 06:04:40.872744       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1130 06:04:40.878648       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1130 06:04:40.880528       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1130 06:04:40.882096       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1130 06:04:40.887746       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1130 06:04:40.900026       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1130 06:04:40.900919       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1130 06:04:40.900958       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1130 06:04:40.900971       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1130 06:04:40.900999       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1130 06:04:40.901292       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1130 06:04:40.902179       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1130 06:04:40.902548       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1130 06:04:40.903403       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1130 06:04:40.903450       1 shared_informer.go:356] "Caches are synced" controller="job"
I1130 06:04:40.903551       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1130 06:04:40.903609       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1130 06:04:40.903617       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1130 06:04:40.904842       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1130 06:04:40.904948       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1130 06:04:40.906547       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1130 06:04:40.907795       1 shared_informer.go:356] "Caches are synced" controller="node"
I1130 06:04:40.907928       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1130 06:04:40.907982       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1130 06:04:40.907990       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1130 06:04:40.907999       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1130 06:04:40.908987       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1130 06:04:40.917735       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1130 06:04:40.918483       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1130 06:04:40.930532       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1130 06:04:40.930570       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1130 06:04:40.930590       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1130 06:04:40.938086       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1130 06:04:40.938708       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1130 06:04:40.938889       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1130 06:04:40.938955       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1130 06:04:40.951419       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1130 06:04:40.951516       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1130 06:04:40.951528       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1130 06:04:40.951660       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1130 06:04:40.951673       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1130 06:04:40.951861       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1130 06:04:40.952977       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1130 06:04:40.953082       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1130 06:04:40.954292       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1130 06:04:40.954315       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1130 06:04:40.954348       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1130 06:04:40.954399       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1130 06:04:40.955507       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1130 06:04:40.955544       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1130 06:04:40.955563       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1130 06:04:40.957880       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1130 06:04:40.960390       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1130 06:04:40.965658       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1130 06:04:40.982838       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-proxy [48b35875e671] <==
I1130 06:04:42.692251       1 server_linux.go:53] "Using iptables proxy"
I1130 06:04:42.791351       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1130 06:04:42.891773       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1130 06:04:42.891830       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1130 06:04:42.892050       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1130 06:04:42.929056       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1130 06:04:42.929162       1 server_linux.go:132] "Using iptables Proxier"
I1130 06:04:42.938936       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1130 06:04:42.939604       1 server.go:527] "Version info" version="v1.34.0"
I1130 06:04:42.939660       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1130 06:04:42.942114       1 config.go:200] "Starting service config controller"
I1130 06:04:42.942145       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1130 06:04:42.942253       1 config.go:106] "Starting endpoint slice config controller"
I1130 06:04:42.943097       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1130 06:04:42.943259       1 config.go:309] "Starting node config controller"
I1130 06:04:42.943680       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1130 06:04:42.943704       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1130 06:04:42.943388       1 config.go:403] "Starting serviceCIDR config controller"
I1130 06:04:42.943745       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1130 06:04:43.043000       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1130 06:04:43.043221       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1130 06:04:43.044411       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [addbce052935] <==
I1130 06:04:32.422646       1 serving.go:386] Generated self-signed cert in-memory
W1130 06:04:33.938377       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1130 06:04:33.938421       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1130 06:04:33.938436       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1130 06:04:33.938449       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1130 06:04:33.955425       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1130 06:04:33.955468       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1130 06:04:33.957411       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1130 06:04:33.957451       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1130 06:04:33.957668       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1130 06:04:33.957713       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1130 06:04:33.959202       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1130 06:04:33.959447       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1130 06:04:33.959579       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1130 06:04:33.959630       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1130 06:04:33.959703       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1130 06:04:33.959572       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1130 06:04:33.959997       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1130 06:04:33.960093       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1130 06:04:33.960187       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1130 06:04:33.960228       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1130 06:04:33.960222       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1130 06:04:33.960221       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1130 06:04:33.960252       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1130 06:04:33.960285       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1130 06:04:33.960491       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1130 06:04:33.960494       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1130 06:04:33.960515       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1130 06:04:33.960660       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1130 06:04:33.960714       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1130 06:04:34.830816       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1130 06:04:34.859361       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1130 06:04:34.987090       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1130 06:04:35.002281       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1130 06:04:35.023599       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1130 06:04:35.023599       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1130 06:04:35.094390       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1130 06:04:35.176343       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1130 06:04:35.203813       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1130 06:04:35.228541       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1130 06:04:35.243004       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
I1130 06:04:37.758380       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Nov 30 06:10:55 minikube kubelet[2477]: E1130 06:10:55.462830    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:11:09 minikube kubelet[2477]: E1130 06:11:09.463500    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:11:19 minikube kubelet[2477]: E1130 06:11:19.483292    2477 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="device-monitor:latest"
Nov 30 06:11:19 minikube kubelet[2477]: E1130 06:11:19.483379    2477 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="device-monitor:latest"
Nov 30 06:11:19 minikube kubelet[2477]: E1130 06:11:19.483511    2477 kuberuntime_manager.go:1449] "Unhandled Error" err="container device-monitor start failed in pod device-monitor-7bd455fb8d-78nhr_default(47ba413a-0570-4eaa-a754-0ddcd0bafe00): ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 30 06:11:19 minikube kubelet[2477]: E1130 06:11:19.483570    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ErrImagePull: \"Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:11:24 minikube kubelet[2477]: E1130 06:11:24.462531    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:11:33 minikube kubelet[2477]: E1130 06:11:33.464348    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:11:36 minikube kubelet[2477]: E1130 06:11:36.465077    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:11:48 minikube kubelet[2477]: E1130 06:11:48.464171    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:11:51 minikube kubelet[2477]: E1130 06:11:51.462655    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:12:00 minikube kubelet[2477]: E1130 06:12:00.464682    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:12:04 minikube kubelet[2477]: E1130 06:12:04.463526    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:12:13 minikube kubelet[2477]: E1130 06:12:13.465236    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:12:16 minikube kubelet[2477]: E1130 06:12:16.463120    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:12:28 minikube kubelet[2477]: E1130 06:12:28.463165    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:12:28 minikube kubelet[2477]: E1130 06:12:28.463652    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:12:41 minikube kubelet[2477]: E1130 06:12:41.463450    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:12:41 minikube kubelet[2477]: E1130 06:12:41.463485    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:12:53 minikube kubelet[2477]: E1130 06:12:53.462924    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:12:54 minikube kubelet[2477]: E1130 06:12:54.464173    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:13:05 minikube kubelet[2477]: E1130 06:13:05.463039    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:13:07 minikube kubelet[2477]: E1130 06:13:07.465298    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:13:18 minikube kubelet[2477]: E1130 06:13:18.464422    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:13:20 minikube kubelet[2477]: E1130 06:13:20.463135    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:13:31 minikube kubelet[2477]: E1130 06:13:31.462588    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:13:38 minikube kubelet[2477]: E1130 06:13:38.712640    2477 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="device-monitor:latest"
Nov 30 06:13:38 minikube kubelet[2477]: E1130 06:13:38.712751    2477 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="device-monitor:latest"
Nov 30 06:13:38 minikube kubelet[2477]: E1130 06:13:38.712923    2477 kuberuntime_manager.go:1449] "Unhandled Error" err="container device-monitor start failed in pod device-monitor-7bd455fb8d-22pbn_default(69103235-0b74-4131-bff1-2bac14cd5746): ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 30 06:13:38 minikube kubelet[2477]: E1130 06:13:38.712990    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ErrImagePull: \"Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:13:46 minikube kubelet[2477]: E1130 06:13:46.462078    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:13:54 minikube kubelet[2477]: E1130 06:13:54.462736    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:13:59 minikube kubelet[2477]: E1130 06:13:59.461895    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:14:07 minikube kubelet[2477]: E1130 06:14:07.461467    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ImagePullBackOff: \"Back-off pulling image \\\"device-monitor:latest\\\": ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-22pbn" podUID="69103235-0b74-4131-bff1-2bac14cd5746"
Nov 30 06:14:16 minikube kubelet[2477]: E1130 06:14:16.644851    2477 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="device-monitor:latest"
Nov 30 06:14:16 minikube kubelet[2477]: E1130 06:14:16.644969    2477 kuberuntime_image.go:43] "Failed to pull image" err="Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="device-monitor:latest"
Nov 30 06:14:16 minikube kubelet[2477]: E1130 06:14:16.645100    2477 kuberuntime_manager.go:1449] "Unhandled Error" err="container device-monitor start failed in pod device-monitor-7bd455fb8d-78nhr_default(47ba413a-0570-4eaa-a754-0ddcd0bafe00): ErrImagePull: Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Nov 30 06:14:16 minikube kubelet[2477]: E1130 06:14:16.645164    2477 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"device-monitor\" with ErrImagePull: \"Error response from daemon: pull access denied for device-monitor, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/device-monitor-7bd455fb8d-78nhr" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00"
Nov 30 06:14:21 minikube kubelet[2477]: I1130 06:14:21.095183    2477 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/device-monitor-7bd455fb8d-22pbn" podStartSLOduration=-9223371626.759613 podStartE2EDuration="6m50.095162417s" podCreationTimestamp="2025-11-30 06:07:31 +0000 UTC" firstStartedPulling="2025-11-30 06:07:31.952886943 +0000 UTC m=+175.603182057" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-30 06:14:21.094945809 +0000 UTC m=+584.745240955" watchObservedRunningTime="2025-11-30 06:14:21.095162417 +0000 UTC m=+584.745457533"
Nov 30 06:14:31 minikube kubelet[2477]: I1130 06:14:31.189463    2477 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/device-monitor-7bd455fb8d-78nhr" podStartSLOduration=-9223371616.665344 podStartE2EDuration="7m0.189431597s" podCreationTimestamp="2025-11-30 06:07:31 +0000 UTC" firstStartedPulling="2025-11-30 06:07:31.952963357 +0000 UTC m=+175.603258471" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-30 06:14:31.189152679 +0000 UTC m=+594.839447803" watchObservedRunningTime="2025-11-30 06:14:31.189431597 +0000 UTC m=+594.839726716"
Nov 30 06:22:11 minikube kubelet[2477]: I1130 06:22:11.608501    2477 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k2nd5\" (UniqueName: \"kubernetes.io/projected/8b69e95f-482a-4334-988f-f42ade1c7546-kube-api-access-k2nd5\") pod \"device-monitor-7d4bcfcccf-cbn9t\" (UID: \"8b69e95f-482a-4334-988f-f42ade1c7546\") " pod="default/device-monitor-7d4bcfcccf-cbn9t"
Nov 30 06:22:18 minikube kubelet[2477]: I1130 06:22:18.846909    2477 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/device-monitor-7d4bcfcccf-cbn9t" podStartSLOduration=7.846857743 podStartE2EDuration="7.846857743s" podCreationTimestamp="2025-11-30 06:22:11 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-30 06:22:12.704515679 +0000 UTC m=+1056.354810801" watchObservedRunningTime="2025-11-30 06:22:18.846857743 +0000 UTC m=+1062.497152863"
Nov 30 06:22:19 minikube kubelet[2477]: I1130 06:22:19.067651    2477 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6wmkr\" (UniqueName: \"kubernetes.io/projected/f4ce07de-7bfc-4e6c-927b-7bbe97078568-kube-api-access-6wmkr\") pod \"device-monitor-7d4bcfcccf-7wgfz\" (UID: \"f4ce07de-7bfc-4e6c-927b-7bbe97078568\") " pod="default/device-monitor-7d4bcfcccf-7wgfz"
Nov 30 06:22:19 minikube kubelet[2477]: I1130 06:22:19.370014    2477 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-drj4n\" (UniqueName: \"kubernetes.io/projected/47ba413a-0570-4eaa-a754-0ddcd0bafe00-kube-api-access-drj4n\") pod \"47ba413a-0570-4eaa-a754-0ddcd0bafe00\" (UID: \"47ba413a-0570-4eaa-a754-0ddcd0bafe00\") "
Nov 30 06:22:19 minikube kubelet[2477]: I1130 06:22:19.373046    2477 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/47ba413a-0570-4eaa-a754-0ddcd0bafe00-kube-api-access-drj4n" (OuterVolumeSpecName: "kube-api-access-drj4n") pod "47ba413a-0570-4eaa-a754-0ddcd0bafe00" (UID: "47ba413a-0570-4eaa-a754-0ddcd0bafe00"). InnerVolumeSpecName "kube-api-access-drj4n". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 30 06:22:19 minikube kubelet[2477]: I1130 06:22:19.470328    2477 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-drj4n\" (UniqueName: \"kubernetes.io/projected/47ba413a-0570-4eaa-a754-0ddcd0bafe00-kube-api-access-drj4n\") on node \"minikube\" DevicePath \"\""
Nov 30 06:22:19 minikube kubelet[2477]: I1130 06:22:19.758622    2477 scope.go:117] "RemoveContainer" containerID="9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae"
Nov 30 06:22:19 minikube kubelet[2477]: I1130 06:22:19.781123    2477 scope.go:117] "RemoveContainer" containerID="9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae"
Nov 30 06:22:19 minikube kubelet[2477]: E1130 06:22:19.782199    2477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae" containerID="9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae"
Nov 30 06:22:19 minikube kubelet[2477]: I1130 06:22:19.782239    2477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae"} err="failed to get container status \"9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae\": rpc error: code = Unknown desc = Error response from daemon: No such container: 9661067cd4c27d5772a0c60eb3889282138533162c691aabf0a6733852d8a7ae"
Nov 30 06:22:20 minikube kubelet[2477]: I1130 06:22:20.472188    2477 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="47ba413a-0570-4eaa-a754-0ddcd0bafe00" path="/var/lib/kubelet/pods/47ba413a-0570-4eaa-a754-0ddcd0bafe00/volumes"
Nov 30 06:22:24 minikube kubelet[2477]: I1130 06:22:24.913286    2477 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/device-monitor-7d4bcfcccf-7wgfz" podStartSLOduration=6.913252633 podStartE2EDuration="6.913252633s" podCreationTimestamp="2025-11-30 06:22:18 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-30 06:22:19.790693585 +0000 UTC m=+1063.440988703" watchObservedRunningTime="2025-11-30 06:22:24.913252633 +0000 UTC m=+1068.563547754"
Nov 30 06:22:25 minikube kubelet[2477]: I1130 06:22:25.417634    2477 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-pc2sr\" (UniqueName: \"kubernetes.io/projected/69103235-0b74-4131-bff1-2bac14cd5746-kube-api-access-pc2sr\") pod \"69103235-0b74-4131-bff1-2bac14cd5746\" (UID: \"69103235-0b74-4131-bff1-2bac14cd5746\") "
Nov 30 06:22:25 minikube kubelet[2477]: I1130 06:22:25.422977    2477 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/69103235-0b74-4131-bff1-2bac14cd5746-kube-api-access-pc2sr" (OuterVolumeSpecName: "kube-api-access-pc2sr") pod "69103235-0b74-4131-bff1-2bac14cd5746" (UID: "69103235-0b74-4131-bff1-2bac14cd5746"). InnerVolumeSpecName "kube-api-access-pc2sr". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Nov 30 06:22:25 minikube kubelet[2477]: I1130 06:22:25.518054    2477 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-pc2sr\" (UniqueName: \"kubernetes.io/projected/69103235-0b74-4131-bff1-2bac14cd5746-kube-api-access-pc2sr\") on node \"minikube\" DevicePath \"\""
Nov 30 06:22:25 minikube kubelet[2477]: I1130 06:22:25.830327    2477 scope.go:117] "RemoveContainer" containerID="d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850"
Nov 30 06:22:25 minikube kubelet[2477]: I1130 06:22:25.844280    2477 scope.go:117] "RemoveContainer" containerID="d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850"
Nov 30 06:22:25 minikube kubelet[2477]: E1130 06:22:25.845537    2477 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850" containerID="d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850"
Nov 30 06:22:25 minikube kubelet[2477]: I1130 06:22:25.845597    2477 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850"} err="failed to get container status \"d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850\": rpc error: code = Unknown desc = Error response from daemon: No such container: d09678b910ada564e98971c0020a9ff2e30e5fa0b3606e0250d8e65314c02850"
Nov 30 06:22:26 minikube kubelet[2477]: I1130 06:22:26.470448    2477 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="69103235-0b74-4131-bff1-2bac14cd5746" path="/var/lib/kubelet/pods/69103235-0b74-4131-bff1-2bac14cd5746/volumes"


==> storage-provisioner [bb24e8b9a130] <==
W1130 06:57:41.292819       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:41.299770       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:43.304548       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:43.309015       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:45.316213       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:45.323607       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:47.327561       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:47.333816       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:49.338524       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:49.342919       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:51.347951       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:51.353245       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:53.357243       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:53.363157       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:55.367706       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:55.373743       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:57.379778       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:57.384467       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:59.387307       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:57:59.392343       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:01.395455       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:01.399009       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:03.402024       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:03.406730       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:05.409603       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:05.415610       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:07.420753       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:07.424088       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:09.430258       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:09.435546       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:11.441675       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:11.446535       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:13.452305       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:13.456570       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:15.460393       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:15.467514       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:17.471636       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:17.476852       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:19.479930       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:19.483695       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:21.487716       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:21.492983       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:23.499333       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:23.505524       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:25.511078       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:25.518217       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:27.521816       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:27.526911       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:29.531444       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:29.536423       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:31.543205       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:31.547762       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:33.555072       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:33.560420       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:35.564925       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:35.570908       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:37.575528       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:37.579693       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:39.584743       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1130 06:58:39.590022       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

